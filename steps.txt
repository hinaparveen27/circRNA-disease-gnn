tried put features... results in feature_results.txt

added an extra hidden layer... due to kernel issues had to restart kernel each time changes made to model

Residual connections from input features to output suppress the contribution of higher-order neighborhood information, causing the model to rely excessively on handcrafted centrality features.
| Setting        | AUC   |
| -------------- | ----- |
| With residuals | ~0.80 |
| No residuals   | ~0.92 |
Removing input-level residual connections significantly improved performance, indicating that deeper message passing is essential for capturing circRNAâ€“disease associations beyond handcrafted node centralities.
Although a four-layer model achieved slightly higher predictive performance, we selected the three-layer architecture as the final model due to its better trade-off between performance, stability, and interpretability. Deeper architectures risk oversmoothing and obscure biological semantics in heterogeneous molecular graphs.

We evaluated different residual connection strategies to assess their impact on heterogeneous message passing. Input-level residuals significantly degraded performance, indicating excessive reliance on node centrality features. Layer-wise residuals partially mitigated this issue but introduced higher variance and reduced predictive performance. The best results were achieved without residual connections, suggesting that explicit relational aggregation is critical for learning biologically meaningful circRNAâ€“disease representations.

We conducted extensive ablation studies to analyze the effects of depth, residual connections, and decoder design. Input-level residuals significantly degraded performance, while layer-wise residuals introduced instability without improving predictive accuracy. Replacing the dot-product decoder with a multi-layer perceptron did not yield consistent gains, indicating that cosine-based similarity is sufficient for modeling circRNAâ€“disease associations. Although a four-layer model achieved the highest performance, we selected a three-layer architecture as the final model due to its superior balance between accuracy, stability, and biological interpretability.
âœ… FINAL MODEL (what you should keep)

3-layer HeteroGraphSAGE, no residuals, dot-product decoder

Why?

Best balance of:

performance

stability

interpretability

Lower variance than deeper models

Strong biological justification

ğŸ”¹ A. 3-layer, NO residuals, dot-product decoder

(baseline / main candidate)

AUC: 0.925 Â± 0.006

AUPR: 0.600 Â± 0.029

Stable

Low variance

Clean semantics

ğŸ”¹ B. 3-layer, input residuals

AUC: 0.798

AUPR: 0.449

Clearly harmful

âœ… Strong negative evidence.

ğŸ”¹ C. 3-layer, layer-wise residuals

AUC: 0.859 Â± 0.031

AUPR: 0.501 Â± 0.034

Higher variance

Worse than no-residuals

âœ… Residuals do not help here.

ğŸ”¹ D. 3-layer, MLP decoder

AUC: 0.897 Â± 0.009

AUPR: 0.574 Â± 0.043

Worse than dot product

Much higher loss variance

âš ï¸ Decoder expressiveness â‰  guaranteed improvement.

ğŸ”¹ E. 4-layer, no residuals, dot-product decoder

(upper bound)

AUC: 0.943 Â± 0.007

AUPR: 0.644 Â± 0.039

Best raw performance

More depth, more mixing


GAT
Created models_gat.py and train_gat.pynb

Noticed that GIP similairty edges is not mentioned explicitly in the model, along with reverse edges.
GIP edges not added in SAGE as well. SHould try with GIP added and removed , both models.


â€œAdding residual connections to a 2-layer heterogeneous GAT significantly degraded performance, suggesting that residual feature injection interferes with attention-based message passing for link prediction in this setting.â€

