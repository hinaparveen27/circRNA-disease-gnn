{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d784bdd2-dbc2-4aeb-8b59-1635f6153480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from models_gat import HeteroGAT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0f5465-e070-4b5b-98b7-5e11f647ac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: C:\\Users\\ayish\\OneDrive\\Documents\\circRNA-disease-gnn\n",
      "DATA_DIR: C:\\Users\\ayish\\OneDrive\\Documents\\circRNA-disease-gnn\\data\\data_cleaned\n",
      "GRAPH_PATH: C:\\Users\\ayish\\OneDrive\\Documents\\circRNA-disease-gnn\\outputs\\data.pt\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE\n",
    "# Notebook-safe root\n",
    "ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = ROOT / \"data\" / \"data_cleaned\"\n",
    "GRAPH_PATH = ROOT / \"outputs\" / \"data.pt\"\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SEEDS = range(42, 47)   # 42, 43, 44, 45, 46\n",
    "EPOCHS = 50\n",
    "LR = 1e-3\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"GRAPH_PATH:\", GRAPH_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c7a5c-227e-4860-9ab0-9ad863624797",
   "metadata": {},
   "source": [
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3007bbf4-81a3-4097-a85c-dc9750ca73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929d76b-17a0-4e48-a39a-a3bef3adf55f",
   "metadata": {},
   "source": [
    "### Load Node Preserving Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f803b51e-a8b2-47e6-9f2c-ae7ff2c13243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(name, le_circ, le_dis):\n",
    "    df = pd.read_csv(DATA_DIR / name)\n",
    "\n",
    "    circ_ids = le_circ.transform(df[\"circRNA\"].astype(str))\n",
    "    dis_ids  = le_dis.transform(df[\"disease\"].astype(str))\n",
    "\n",
    "    edges = torch.from_numpy(\n",
    "        np.vstack([circ_ids, dis_ids])\n",
    "    ).long()\n",
    "\n",
    "    labels = torch.tensor(df[\"label\"].values, dtype=torch.float)\n",
    "\n",
    "    return edges, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1e344b-d99f-4e5a-b53f-db6635a06e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 929]), torch.Size([929]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders = torch.load(\n",
    "    OUT_DIR / \"label_encoders.pt\",\n",
    "    weights_only=False\n",
    ")\n",
    "\n",
    "le_circ = encoders[\"circRNA\"]\n",
    "le_dis  = encoders[\"disease\"]\n",
    "\n",
    "train_edges, train_labels = load_split(\n",
    "    \"circRNA_disease_train.csv\", le_circ, le_dis\n",
    ")\n",
    "val_edges, val_labels = load_split(\n",
    "    \"circRNA_disease_val.csv\", le_circ, le_dis\n",
    ")\n",
    "test_edges, test_labels = load_split(\n",
    "    \"circRNA_disease_test.csv\", le_circ, le_dis\n",
    ")\n",
    "train_edges.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf748fc-7f40-456e-991f-4f7c511514db",
   "metadata": {},
   "source": [
    "### Move Splits to Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d59aea-cd31-4e89-976e-6ac1b88987c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edges, train_labels = train_edges.to(DEVICE), train_labels.to(DEVICE)\n",
    "val_edges, val_labels     = val_edges.to(DEVICE), val_labels.to(DEVICE)\n",
    "test_edges, test_labels   = test_edges.to(DEVICE), test_labels.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e201ed-5a32-4bce-a9c4-f1689f074e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading heterogeneous graph...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  circRNA={ x=[828, 6] },\n",
       "  miRNA={ x=[521, 6] },\n",
       "  disease={ x=[122, 6] },\n",
       "  (circRNA, interacts, miRNA)={ edge_index=[2, 896] },\n",
       "  (miRNA, interacts, disease)={ edge_index=[2, 828] },\n",
       "  (circRNA, associated, disease)={ edge_index=[2, 985] },\n",
       "  (circRNA, gip_sim, circRNA)={\n",
       "    edge_index=[2, 685584],\n",
       "    edge_weight=[685584],\n",
       "  },\n",
       "  (miRNA, gip_sim, miRNA)={\n",
       "    edge_index=[2, 271441],\n",
       "    edge_weight=[271441],\n",
       "  },\n",
       "  (miRNA, rev_interacts, circRNA)={ edge_index=[2, 896] },\n",
       "  (disease, rev_interacts, miRNA)={ edge_index=[2, 828] },\n",
       "  (disease, rev_associated, circRNA)={ edge_index=[2, 985] }\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading heterogeneous graph...\")\n",
    "data = torch.load(\n",
    "    GRAPH_PATH,\n",
    "    map_location=DEVICE,\n",
    "    weights_only=False\n",
    ")\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c7fc1b7-8b52-486d-94ae-8eb287068fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('circRNA', 'interacts', 'miRNA'), ('miRNA', 'interacts', 'disease'), ('circRNA', 'associated', 'disease'), ('circRNA', 'gip_sim', 'circRNA'), ('miRNA', 'gip_sim', 'miRNA'), ('miRNA', 'rev_interacts', 'circRNA'), ('disease', 'rev_interacts', 'miRNA'), ('disease', 'rev_associated', 'circRNA')]\n"
     ]
    }
   ],
   "source": [
    "print(data.edge_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b66b00-75c6-408f-a041-2d4d7b4a0433",
   "metadata": {},
   "source": [
    "### Multi Seed Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74f4e712-6be6-4776-8874-180413f59582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "Running GAT experiment | SEED = 42\n",
      "===================================\n",
      "Epoch 001 | Loss 0.7528 | Val AUC 0.5167 | Val AUPR 0.1132\n",
      "   → Saved best model\n",
      "Epoch 002 | Loss 0.6458 | Val AUC 0.7669 | Val AUPR 0.2361\n",
      "   → Saved best model\n",
      "Epoch 003 | Loss 0.5601 | Val AUC 0.8367 | Val AUPR 0.3449\n",
      "   → Saved best model\n",
      "Epoch 004 | Loss 0.5045 | Val AUC 0.8724 | Val AUPR 0.4228\n",
      "   → Saved best model\n",
      "Epoch 005 | Loss 0.4643 | Val AUC 0.8941 | Val AUPR 0.4984\n",
      "   → Saved best model\n",
      "Epoch 006 | Loss 0.4429 | Val AUC 0.9099 | Val AUPR 0.6018\n",
      "   → Saved best model\n",
      "Epoch 007 | Loss 0.4290 | Val AUC 0.9181 | Val AUPR 0.6369\n",
      "   → Saved best model\n",
      "Epoch 008 | Loss 0.4200 | Val AUC 0.9212 | Val AUPR 0.6554\n",
      "   → Saved best model\n",
      "Epoch 009 | Loss 0.4165 | Val AUC 0.9224 | Val AUPR 0.6636\n",
      "   → Saved best model\n",
      "Epoch 010 | Loss 0.4118 | Val AUC 0.9207 | Val AUPR 0.6562\n",
      "Epoch 011 | Loss 0.4103 | Val AUC 0.9185 | Val AUPR 0.6437\n",
      "Epoch 012 | Loss 0.4067 | Val AUC 0.9159 | Val AUPR 0.6284\n",
      "Epoch 013 | Loss 0.4044 | Val AUC 0.9140 | Val AUPR 0.6127\n",
      "Epoch 014 | Loss 0.4025 | Val AUC 0.9120 | Val AUPR 0.5893\n",
      "Epoch 015 | Loss 0.3997 | Val AUC 0.9108 | Val AUPR 0.5722\n",
      "Epoch 016 | Loss 0.3982 | Val AUC 0.9101 | Val AUPR 0.5428\n",
      "Epoch 017 | Loss 0.3962 | Val AUC 0.9110 | Val AUPR 0.5415\n",
      "Epoch 018 | Loss 0.3946 | Val AUC 0.9103 | Val AUPR 0.5280\n",
      "Epoch 019 | Loss 0.3935 | Val AUC 0.9098 | Val AUPR 0.5163\n",
      "Epoch 020 | Loss 0.3918 | Val AUC 0.9091 | Val AUPR 0.5105\n",
      "Epoch 021 | Loss 0.3912 | Val AUC 0.9084 | Val AUPR 0.5030\n",
      "Epoch 022 | Loss 0.3901 | Val AUC 0.9074 | Val AUPR 0.4905\n",
      "Epoch 023 | Loss 0.3894 | Val AUC 0.9059 | Val AUPR 0.4787\n",
      "Epoch 024 | Loss 0.3890 | Val AUC 0.9046 | Val AUPR 0.4696\n",
      "Epoch 025 | Loss 0.3878 | Val AUC 0.9020 | Val AUPR 0.4518\n",
      "Epoch 026 | Loss 0.3880 | Val AUC 0.8987 | Val AUPR 0.4340\n",
      "Epoch 027 | Loss 0.3870 | Val AUC 0.8905 | Val AUPR 0.4105\n",
      "Epoch 028 | Loss 0.3865 | Val AUC 0.8857 | Val AUPR 0.4065\n",
      "Epoch 029 | Loss 0.3855 | Val AUC 0.8844 | Val AUPR 0.4003\n",
      "Epoch 030 | Loss 0.3857 | Val AUC 0.8846 | Val AUPR 0.4022\n",
      "Epoch 031 | Loss 0.3854 | Val AUC 0.8855 | Val AUPR 0.4087\n",
      "Epoch 032 | Loss 0.3854 | Val AUC 0.8871 | Val AUPR 0.4222\n",
      "Epoch 033 | Loss 0.3848 | Val AUC 0.8884 | Val AUPR 0.4281\n",
      "Epoch 034 | Loss 0.3840 | Val AUC 0.8907 | Val AUPR 0.4344\n",
      "Epoch 035 | Loss 0.3837 | Val AUC 0.8928 | Val AUPR 0.4427\n",
      "Epoch 036 | Loss 0.3837 | Val AUC 0.8948 | Val AUPR 0.4508\n",
      "Epoch 037 | Loss 0.3832 | Val AUC 0.8972 | Val AUPR 0.4598\n",
      "Epoch 038 | Loss 0.3831 | Val AUC 0.8992 | Val AUPR 0.4673\n",
      "Epoch 039 | Loss 0.3822 | Val AUC 0.9015 | Val AUPR 0.4734\n",
      "Epoch 040 | Loss 0.3819 | Val AUC 0.9033 | Val AUPR 0.4840\n",
      "Epoch 041 | Loss 0.3823 | Val AUC 0.9048 | Val AUPR 0.4906\n",
      "Epoch 042 | Loss 0.3816 | Val AUC 0.9061 | Val AUPR 0.4991\n",
      "Epoch 043 | Loss 0.3809 | Val AUC 0.9079 | Val AUPR 0.5052\n",
      "Epoch 044 | Loss 0.3810 | Val AUC 0.9094 | Val AUPR 0.5134\n",
      "Epoch 045 | Loss 0.3812 | Val AUC 0.9109 | Val AUPR 0.5194\n",
      "Epoch 046 | Loss 0.3811 | Val AUC 0.9119 | Val AUPR 0.5238\n",
      "Epoch 047 | Loss 0.3805 | Val AUC 0.9126 | Val AUPR 0.5247\n",
      "Epoch 048 | Loss 0.3806 | Val AUC 0.9131 | Val AUPR 0.5175\n",
      "Epoch 049 | Loss 0.3799 | Val AUC 0.9136 | Val AUPR 0.5169\n",
      "Epoch 050 | Loss 0.3791 | Val AUC 0.9141 | Val AUPR 0.5152\n",
      "SEED 42 | Test AUC 0.9243 | Test AUPR 0.5617\n",
      "\n",
      "===================================\n",
      "Running GAT experiment | SEED = 43\n",
      "===================================\n",
      "Epoch 001 | Loss 0.6966 | Val AUC 0.8230 | Val AUPR 0.3188\n",
      "   → Saved best model\n",
      "Epoch 002 | Loss 0.6034 | Val AUC 0.8590 | Val AUPR 0.3684\n",
      "   → Saved best model\n",
      "Epoch 003 | Loss 0.5417 | Val AUC 0.8690 | Val AUPR 0.3976\n",
      "   → Saved best model\n",
      "Epoch 004 | Loss 0.4938 | Val AUC 0.8759 | Val AUPR 0.4175\n",
      "   → Saved best model\n",
      "Epoch 005 | Loss 0.4591 | Val AUC 0.8788 | Val AUPR 0.4339\n",
      "   → Saved best model\n",
      "Epoch 006 | Loss 0.4365 | Val AUC 0.8783 | Val AUPR 0.4332\n",
      "Epoch 007 | Loss 0.4251 | Val AUC 0.8735 | Val AUPR 0.4185\n",
      "Epoch 008 | Loss 0.4182 | Val AUC 0.8687 | Val AUPR 0.4028\n",
      "Epoch 009 | Loss 0.4127 | Val AUC 0.8705 | Val AUPR 0.4041\n",
      "Epoch 010 | Loss 0.4068 | Val AUC 0.8831 | Val AUPR 0.4381\n",
      "   → Saved best model\n",
      "Epoch 011 | Loss 0.4063 | Val AUC 0.8905 | Val AUPR 0.4752\n",
      "   → Saved best model\n",
      "Epoch 012 | Loss 0.4041 | Val AUC 0.8963 | Val AUPR 0.5044\n",
      "   → Saved best model\n",
      "Epoch 013 | Loss 0.4028 | Val AUC 0.9017 | Val AUPR 0.5510\n",
      "   → Saved best model\n",
      "Epoch 014 | Loss 0.3998 | Val AUC 0.9060 | Val AUPR 0.5636\n",
      "   → Saved best model\n",
      "Epoch 015 | Loss 0.3989 | Val AUC 0.9103 | Val AUPR 0.5741\n",
      "   → Saved best model\n",
      "Epoch 016 | Loss 0.3965 | Val AUC 0.9140 | Val AUPR 0.5887\n",
      "   → Saved best model\n",
      "Epoch 017 | Loss 0.3936 | Val AUC 0.9177 | Val AUPR 0.6058\n",
      "   → Saved best model\n",
      "Epoch 018 | Loss 0.3928 | Val AUC 0.9205 | Val AUPR 0.6202\n",
      "   → Saved best model\n",
      "Epoch 019 | Loss 0.3916 | Val AUC 0.9222 | Val AUPR 0.6295\n",
      "   → Saved best model\n",
      "Epoch 020 | Loss 0.3898 | Val AUC 0.9239 | Val AUPR 0.6377\n",
      "   → Saved best model\n",
      "Epoch 021 | Loss 0.3892 | Val AUC 0.9246 | Val AUPR 0.6361\n",
      "Epoch 022 | Loss 0.3878 | Val AUC 0.9244 | Val AUPR 0.6330\n",
      "Epoch 023 | Loss 0.3886 | Val AUC 0.9231 | Val AUPR 0.6172\n",
      "Epoch 024 | Loss 0.3872 | Val AUC 0.9217 | Val AUPR 0.6090\n",
      "Epoch 025 | Loss 0.3861 | Val AUC 0.9194 | Val AUPR 0.6025\n",
      "Epoch 026 | Loss 0.3863 | Val AUC 0.9188 | Val AUPR 0.5996\n",
      "Epoch 027 | Loss 0.3859 | Val AUC 0.9178 | Val AUPR 0.5902\n",
      "Epoch 028 | Loss 0.3854 | Val AUC 0.9168 | Val AUPR 0.5791\n",
      "Epoch 029 | Loss 0.3847 | Val AUC 0.9162 | Val AUPR 0.5709\n",
      "Epoch 030 | Loss 0.3848 | Val AUC 0.9152 | Val AUPR 0.5612\n",
      "Epoch 031 | Loss 0.3841 | Val AUC 0.9154 | Val AUPR 0.5574\n",
      "Epoch 032 | Loss 0.3831 | Val AUC 0.9156 | Val AUPR 0.5587\n",
      "Epoch 033 | Loss 0.3836 | Val AUC 0.9161 | Val AUPR 0.5625\n",
      "Epoch 034 | Loss 0.3834 | Val AUC 0.9165 | Val AUPR 0.5683\n",
      "Epoch 035 | Loss 0.3825 | Val AUC 0.9164 | Val AUPR 0.5707\n",
      "Epoch 036 | Loss 0.3823 | Val AUC 0.9165 | Val AUPR 0.5721\n",
      "Epoch 037 | Loss 0.3811 | Val AUC 0.9167 | Val AUPR 0.5706\n",
      "Epoch 038 | Loss 0.3817 | Val AUC 0.9164 | Val AUPR 0.5611\n",
      "Epoch 039 | Loss 0.3804 | Val AUC 0.9167 | Val AUPR 0.5602\n",
      "Epoch 040 | Loss 0.3809 | Val AUC 0.9166 | Val AUPR 0.5579\n",
      "Epoch 041 | Loss 0.3804 | Val AUC 0.9164 | Val AUPR 0.5546\n",
      "Epoch 042 | Loss 0.3800 | Val AUC 0.9160 | Val AUPR 0.5505\n",
      "Epoch 043 | Loss 0.3799 | Val AUC 0.9154 | Val AUPR 0.5412\n",
      "Epoch 044 | Loss 0.3793 | Val AUC 0.9151 | Val AUPR 0.5367\n",
      "Epoch 045 | Loss 0.3791 | Val AUC 0.9147 | Val AUPR 0.5332\n",
      "Epoch 046 | Loss 0.3790 | Val AUC 0.9149 | Val AUPR 0.5328\n",
      "Epoch 047 | Loss 0.3775 | Val AUC 0.9151 | Val AUPR 0.5351\n",
      "Epoch 048 | Loss 0.3775 | Val AUC 0.9152 | Val AUPR 0.5363\n",
      "Epoch 049 | Loss 0.3781 | Val AUC 0.9156 | Val AUPR 0.5368\n",
      "Epoch 050 | Loss 0.3773 | Val AUC 0.9156 | Val AUPR 0.5372\n",
      "SEED 43 | Test AUC 0.9403 | Test AUPR 0.6851\n",
      "\n",
      "===================================\n",
      "Running GAT experiment | SEED = 44\n",
      "===================================\n",
      "Epoch 001 | Loss 0.6646 | Val AUC 0.5992 | Val AUPR 0.1300\n",
      "   → Saved best model\n",
      "Epoch 002 | Loss 0.5809 | Val AUC 0.7079 | Val AUPR 0.1713\n",
      "   → Saved best model\n",
      "Epoch 003 | Loss 0.5164 | Val AUC 0.7426 | Val AUPR 0.1931\n",
      "   → Saved best model\n",
      "Epoch 004 | Loss 0.4710 | Val AUC 0.7770 | Val AUPR 0.2222\n",
      "   → Saved best model\n",
      "Epoch 005 | Loss 0.4481 | Val AUC 0.8262 | Val AUPR 0.2834\n",
      "   → Saved best model\n",
      "Epoch 006 | Loss 0.4314 | Val AUC 0.8603 | Val AUPR 0.3353\n",
      "   → Saved best model\n",
      "Epoch 007 | Loss 0.4218 | Val AUC 0.8781 | Val AUPR 0.3846\n",
      "   → Saved best model\n",
      "Epoch 008 | Loss 0.4174 | Val AUC 0.8913 | Val AUPR 0.4465\n",
      "   → Saved best model\n",
      "Epoch 009 | Loss 0.4143 | Val AUC 0.9006 | Val AUPR 0.4986\n",
      "   → Saved best model\n",
      "Epoch 010 | Loss 0.4063 | Val AUC 0.9076 | Val AUPR 0.5675\n",
      "   → Saved best model\n",
      "Epoch 011 | Loss 0.4068 | Val AUC 0.9121 | Val AUPR 0.6072\n",
      "   → Saved best model\n",
      "Epoch 012 | Loss 0.4037 | Val AUC 0.9159 | Val AUPR 0.6201\n",
      "   → Saved best model\n",
      "Epoch 013 | Loss 0.4008 | Val AUC 0.9171 | Val AUPR 0.6241\n",
      "   → Saved best model\n",
      "Epoch 014 | Loss 0.3972 | Val AUC 0.9190 | Val AUPR 0.6304\n",
      "   → Saved best model\n",
      "Epoch 015 | Loss 0.3967 | Val AUC 0.9196 | Val AUPR 0.6234\n",
      "Epoch 016 | Loss 0.3946 | Val AUC 0.9211 | Val AUPR 0.6261\n",
      "Epoch 017 | Loss 0.3929 | Val AUC 0.9222 | Val AUPR 0.6394\n",
      "   → Saved best model\n",
      "Epoch 018 | Loss 0.3921 | Val AUC 0.9238 | Val AUPR 0.6437\n",
      "   → Saved best model\n",
      "Epoch 019 | Loss 0.3913 | Val AUC 0.9248 | Val AUPR 0.6569\n",
      "   → Saved best model\n",
      "Epoch 020 | Loss 0.3897 | Val AUC 0.9254 | Val AUPR 0.6601\n",
      "   → Saved best model\n",
      "Epoch 021 | Loss 0.3892 | Val AUC 0.9254 | Val AUPR 0.6577\n",
      "Epoch 022 | Loss 0.3876 | Val AUC 0.9247 | Val AUPR 0.6538\n",
      "Epoch 023 | Loss 0.3873 | Val AUC 0.9232 | Val AUPR 0.6488\n",
      "Epoch 024 | Loss 0.3865 | Val AUC 0.9212 | Val AUPR 0.6194\n",
      "Epoch 025 | Loss 0.3866 | Val AUC 0.9196 | Val AUPR 0.5880\n",
      "Epoch 026 | Loss 0.3851 | Val AUC 0.9175 | Val AUPR 0.5889\n",
      "Epoch 027 | Loss 0.3850 | Val AUC 0.9161 | Val AUPR 0.5847\n",
      "Epoch 028 | Loss 0.3840 | Val AUC 0.9147 | Val AUPR 0.5828\n",
      "Epoch 029 | Loss 0.3843 | Val AUC 0.9134 | Val AUPR 0.5741\n",
      "Epoch 030 | Loss 0.3837 | Val AUC 0.9134 | Val AUPR 0.5699\n",
      "Epoch 031 | Loss 0.3829 | Val AUC 0.9132 | Val AUPR 0.5678\n",
      "Epoch 032 | Loss 0.3827 | Val AUC 0.9127 | Val AUPR 0.5646\n",
      "Epoch 033 | Loss 0.3823 | Val AUC 0.9122 | Val AUPR 0.5585\n",
      "Epoch 034 | Loss 0.3824 | Val AUC 0.9122 | Val AUPR 0.5532\n",
      "Epoch 035 | Loss 0.3815 | Val AUC 0.9127 | Val AUPR 0.5521\n",
      "Epoch 036 | Loss 0.3817 | Val AUC 0.9137 | Val AUPR 0.5499\n",
      "Epoch 037 | Loss 0.3805 | Val AUC 0.9154 | Val AUPR 0.5556\n",
      "Epoch 038 | Loss 0.3803 | Val AUC 0.9170 | Val AUPR 0.5612\n",
      "Epoch 039 | Loss 0.3801 | Val AUC 0.9185 | Val AUPR 0.5745\n",
      "Epoch 040 | Loss 0.3792 | Val AUC 0.9201 | Val AUPR 0.5803\n",
      "Epoch 041 | Loss 0.3795 | Val AUC 0.9207 | Val AUPR 0.5811\n",
      "Epoch 042 | Loss 0.3795 | Val AUC 0.9212 | Val AUPR 0.5848\n",
      "Epoch 043 | Loss 0.3786 | Val AUC 0.9220 | Val AUPR 0.5927\n",
      "Epoch 044 | Loss 0.3779 | Val AUC 0.9225 | Val AUPR 0.6011\n",
      "Epoch 045 | Loss 0.3776 | Val AUC 0.9228 | Val AUPR 0.6087\n",
      "Epoch 046 | Loss 0.3769 | Val AUC 0.9231 | Val AUPR 0.6116\n",
      "Epoch 047 | Loss 0.3769 | Val AUC 0.9230 | Val AUPR 0.6123\n",
      "Epoch 048 | Loss 0.3768 | Val AUC 0.9227 | Val AUPR 0.6212\n",
      "Epoch 049 | Loss 0.3754 | Val AUC 0.9226 | Val AUPR 0.6232\n",
      "Epoch 050 | Loss 0.3759 | Val AUC 0.9227 | Val AUPR 0.6277\n",
      "SEED 44 | Test AUC 0.9227 | Test AUPR 0.6061\n",
      "\n",
      "===================================\n",
      "Running GAT experiment | SEED = 45\n",
      "===================================\n",
      "Epoch 001 | Loss 0.6956 | Val AUC 0.7973 | Val AUPR 0.2558\n",
      "   → Saved best model\n",
      "Epoch 002 | Loss 0.6028 | Val AUC 0.8845 | Val AUPR 0.4423\n",
      "   → Saved best model\n",
      "Epoch 003 | Loss 0.5392 | Val AUC 0.9028 | Val AUPR 0.5331\n",
      "   → Saved best model\n",
      "Epoch 004 | Loss 0.4927 | Val AUC 0.9113 | Val AUPR 0.5980\n",
      "   → Saved best model\n",
      "Epoch 005 | Loss 0.4662 | Val AUC 0.9170 | Val AUPR 0.6421\n",
      "   → Saved best model\n",
      "Epoch 006 | Loss 0.4475 | Val AUC 0.9125 | Val AUPR 0.5972\n",
      "Epoch 007 | Loss 0.4372 | Val AUC 0.8931 | Val AUPR 0.4693\n",
      "Epoch 008 | Loss 0.4288 | Val AUC 0.8777 | Val AUPR 0.4065\n",
      "Epoch 009 | Loss 0.4212 | Val AUC 0.8688 | Val AUPR 0.3654\n",
      "Epoch 010 | Loss 0.4158 | Val AUC 0.8696 | Val AUPR 0.3645\n",
      "Epoch 011 | Loss 0.4115 | Val AUC 0.8751 | Val AUPR 0.3732\n",
      "Epoch 012 | Loss 0.4092 | Val AUC 0.8817 | Val AUPR 0.3861\n",
      "Epoch 013 | Loss 0.4062 | Val AUC 0.8816 | Val AUPR 0.3865\n",
      "Epoch 014 | Loss 0.4035 | Val AUC 0.8792 | Val AUPR 0.3822\n",
      "Epoch 015 | Loss 0.4001 | Val AUC 0.8768 | Val AUPR 0.3793\n",
      "Epoch 016 | Loss 0.3987 | Val AUC 0.8754 | Val AUPR 0.3814\n",
      "Epoch 017 | Loss 0.3964 | Val AUC 0.8743 | Val AUPR 0.3865\n",
      "Epoch 018 | Loss 0.3961 | Val AUC 0.8722 | Val AUPR 0.3796\n",
      "Epoch 019 | Loss 0.3936 | Val AUC 0.8707 | Val AUPR 0.3758\n",
      "Epoch 020 | Loss 0.3928 | Val AUC 0.8752 | Val AUPR 0.3932\n",
      "Epoch 021 | Loss 0.3908 | Val AUC 0.8833 | Val AUPR 0.4391\n",
      "Epoch 022 | Loss 0.3906 | Val AUC 0.8899 | Val AUPR 0.4698\n",
      "Epoch 023 | Loss 0.3893 | Val AUC 0.8942 | Val AUPR 0.4953\n",
      "Epoch 024 | Loss 0.3887 | Val AUC 0.8974 | Val AUPR 0.5027\n",
      "Epoch 025 | Loss 0.3886 | Val AUC 0.8991 | Val AUPR 0.5038\n",
      "Epoch 026 | Loss 0.3883 | Val AUC 0.9011 | Val AUPR 0.5101\n",
      "Epoch 027 | Loss 0.3873 | Val AUC 0.9017 | Val AUPR 0.5092\n",
      "Epoch 028 | Loss 0.3873 | Val AUC 0.9022 | Val AUPR 0.5172\n",
      "Epoch 029 | Loss 0.3859 | Val AUC 0.9024 | Val AUPR 0.5239\n",
      "Epoch 030 | Loss 0.3859 | Val AUC 0.9026 | Val AUPR 0.5277\n",
      "Epoch 031 | Loss 0.3851 | Val AUC 0.9037 | Val AUPR 0.5266\n",
      "Epoch 032 | Loss 0.3846 | Val AUC 0.9045 | Val AUPR 0.5294\n",
      "Epoch 033 | Loss 0.3851 | Val AUC 0.9055 | Val AUPR 0.5347\n",
      "Epoch 034 | Loss 0.3846 | Val AUC 0.9075 | Val AUPR 0.5416\n",
      "Epoch 035 | Loss 0.3837 | Val AUC 0.9091 | Val AUPR 0.5422\n",
      "Epoch 036 | Loss 0.3831 | Val AUC 0.9107 | Val AUPR 0.5428\n",
      "Epoch 037 | Loss 0.3835 | Val AUC 0.9126 | Val AUPR 0.5444\n",
      "Epoch 038 | Loss 0.3824 | Val AUC 0.9141 | Val AUPR 0.5480\n",
      "Epoch 039 | Loss 0.3827 | Val AUC 0.9157 | Val AUPR 0.5521\n",
      "Epoch 040 | Loss 0.3816 | Val AUC 0.9177 | Val AUPR 0.5618\n",
      "Epoch 041 | Loss 0.3825 | Val AUC 0.9197 | Val AUPR 0.5659\n",
      "Epoch 042 | Loss 0.3822 | Val AUC 0.9213 | Val AUPR 0.5786\n",
      "Epoch 043 | Loss 0.3814 | Val AUC 0.9224 | Val AUPR 0.5814\n",
      "Epoch 044 | Loss 0.3805 | Val AUC 0.9232 | Val AUPR 0.5849\n",
      "Epoch 045 | Loss 0.3802 | Val AUC 0.9241 | Val AUPR 0.5920\n",
      "Epoch 046 | Loss 0.3804 | Val AUC 0.9249 | Val AUPR 0.5987\n",
      "Epoch 047 | Loss 0.3801 | Val AUC 0.9254 | Val AUPR 0.6034\n",
      "Epoch 048 | Loss 0.3794 | Val AUC 0.9261 | Val AUPR 0.6080\n",
      "Epoch 049 | Loss 0.3791 | Val AUC 0.9262 | Val AUPR 0.6054\n",
      "Epoch 050 | Loss 0.3792 | Val AUC 0.9259 | Val AUPR 0.6011\n",
      "SEED 45 | Test AUC 0.9349 | Test AUPR 0.5920\n",
      "\n",
      "===================================\n",
      "Running GAT experiment | SEED = 46\n",
      "===================================\n",
      "Epoch 001 | Loss 0.7090 | Val AUC 0.5011 | Val AUPR 0.1072\n",
      "   → Saved best model\n",
      "Epoch 002 | Loss 0.6062 | Val AUC 0.7738 | Val AUPR 0.2382\n",
      "   → Saved best model\n",
      "Epoch 003 | Loss 0.5317 | Val AUC 0.8353 | Val AUPR 0.3100\n",
      "   → Saved best model\n",
      "Epoch 004 | Loss 0.4780 | Val AUC 0.8564 | Val AUPR 0.3608\n",
      "   → Saved best model\n",
      "Epoch 005 | Loss 0.4528 | Val AUC 0.8777 | Val AUPR 0.4234\n",
      "   → Saved best model\n",
      "Epoch 006 | Loss 0.4385 | Val AUC 0.8965 | Val AUPR 0.5091\n",
      "   → Saved best model\n",
      "Epoch 007 | Loss 0.4282 | Val AUC 0.9092 | Val AUPR 0.6104\n",
      "   → Saved best model\n",
      "Epoch 008 | Loss 0.4225 | Val AUC 0.9194 | Val AUPR 0.6700\n",
      "   → Saved best model\n",
      "Epoch 009 | Loss 0.4173 | Val AUC 0.9260 | Val AUPR 0.6990\n",
      "   → Saved best model\n",
      "Epoch 010 | Loss 0.4132 | Val AUC 0.9303 | Val AUPR 0.7061\n",
      "   → Saved best model\n",
      "Epoch 011 | Loss 0.4110 | Val AUC 0.9345 | Val AUPR 0.6991\n",
      "Epoch 012 | Loss 0.4068 | Val AUC 0.9360 | Val AUPR 0.6674\n",
      "Epoch 013 | Loss 0.4048 | Val AUC 0.9382 | Val AUPR 0.6263\n",
      "Epoch 014 | Loss 0.4023 | Val AUC 0.9401 | Val AUPR 0.6176\n",
      "Epoch 015 | Loss 0.3997 | Val AUC 0.9421 | Val AUPR 0.6277\n",
      "Epoch 016 | Loss 0.3980 | Val AUC 0.9437 | Val AUPR 0.6786\n",
      "Epoch 017 | Loss 0.3977 | Val AUC 0.9449 | Val AUPR 0.6975\n",
      "Epoch 018 | Loss 0.3954 | Val AUC 0.9471 | Val AUPR 0.7023\n",
      "Epoch 019 | Loss 0.3943 | Val AUC 0.9473 | Val AUPR 0.6729\n",
      "Epoch 020 | Loss 0.3934 | Val AUC 0.9477 | Val AUPR 0.6585\n",
      "Epoch 021 | Loss 0.3928 | Val AUC 0.9461 | Val AUPR 0.6436\n",
      "Epoch 022 | Loss 0.3905 | Val AUC 0.9455 | Val AUPR 0.6405\n",
      "Epoch 023 | Loss 0.3909 | Val AUC 0.9447 | Val AUPR 0.6460\n",
      "Epoch 024 | Loss 0.3891 | Val AUC 0.9427 | Val AUPR 0.6390\n",
      "Epoch 025 | Loss 0.3885 | Val AUC 0.9400 | Val AUPR 0.6309\n",
      "Epoch 026 | Loss 0.3885 | Val AUC 0.9372 | Val AUPR 0.6214\n",
      "Epoch 027 | Loss 0.3876 | Val AUC 0.9348 | Val AUPR 0.6258\n",
      "Epoch 028 | Loss 0.3866 | Val AUC 0.9317 | Val AUPR 0.6148\n",
      "Epoch 029 | Loss 0.3871 | Val AUC 0.9292 | Val AUPR 0.6182\n",
      "Epoch 030 | Loss 0.3867 | Val AUC 0.9262 | Val AUPR 0.6025\n",
      "Epoch 031 | Loss 0.3858 | Val AUC 0.9239 | Val AUPR 0.6062\n",
      "Epoch 032 | Loss 0.3858 | Val AUC 0.9222 | Val AUPR 0.6003\n",
      "Epoch 033 | Loss 0.3853 | Val AUC 0.9215 | Val AUPR 0.6019\n",
      "Epoch 034 | Loss 0.3847 | Val AUC 0.9207 | Val AUPR 0.5998\n",
      "Epoch 035 | Loss 0.3852 | Val AUC 0.9205 | Val AUPR 0.5930\n",
      "Epoch 036 | Loss 0.3844 | Val AUC 0.9204 | Val AUPR 0.5863\n",
      "Epoch 037 | Loss 0.3843 | Val AUC 0.9204 | Val AUPR 0.5855\n",
      "Epoch 038 | Loss 0.3837 | Val AUC 0.9210 | Val AUPR 0.5858\n",
      "Epoch 039 | Loss 0.3832 | Val AUC 0.9211 | Val AUPR 0.5862\n",
      "Epoch 040 | Loss 0.3833 | Val AUC 0.9215 | Val AUPR 0.5837\n",
      "Epoch 041 | Loss 0.3833 | Val AUC 0.9220 | Val AUPR 0.5834\n",
      "Epoch 042 | Loss 0.3830 | Val AUC 0.9226 | Val AUPR 0.5853\n",
      "Epoch 043 | Loss 0.3826 | Val AUC 0.9226 | Val AUPR 0.5774\n",
      "Epoch 044 | Loss 0.3819 | Val AUC 0.9228 | Val AUPR 0.5716\n",
      "Epoch 045 | Loss 0.3819 | Val AUC 0.9227 | Val AUPR 0.5649\n",
      "Epoch 046 | Loss 0.3813 | Val AUC 0.9224 | Val AUPR 0.5549\n",
      "Epoch 047 | Loss 0.3820 | Val AUC 0.9222 | Val AUPR 0.5460\n",
      "Epoch 048 | Loss 0.3814 | Val AUC 0.9220 | Val AUPR 0.5442\n",
      "Epoch 049 | Loss 0.3809 | Val AUC 0.9222 | Val AUPR 0.5450\n",
      "Epoch 050 | Loss 0.3809 | Val AUC 0.9223 | Val AUPR 0.5432\n",
      "SEED 46 | Test AUC 0.9223 | Test AUPR 0.6335\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(\"\\n===================================\")\n",
    "    print(f\"Running GAT experiment | SEED = {seed}\")\n",
    "    print(\"===================================\")\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = HeteroGAT(\n",
    "        in_channels=data[\"circRNA\"].x.size(1),\n",
    "        hidden_channels=64,\n",
    "        out_channels=64,\n",
    "        heads=4,\n",
    "        dropout=0.2\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_aupr = 0.0\n",
    "\n",
    "    # -------- Training --------\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        emb = model(data.x_dict, data.edge_index_dict)\n",
    "        circ_emb, dis_emb = emb[\"circRNA\"], emb[\"disease\"]\n",
    "\n",
    "        logits = (circ_emb[train_edges[0]] * dis_emb[train_edges[1]]).sum(dim=1)\n",
    "        loss = loss_fn(logits, train_labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # -------- Validation --------\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            emb = model(data.x_dict, data.edge_index_dict)\n",
    "            circ_emb, dis_emb = emb[\"circRNA\"], emb[\"disease\"]\n",
    "\n",
    "            val_logits = (circ_emb[val_edges[0]] * dis_emb[val_edges[1]]).sum(dim=1)\n",
    "            val_scores = torch.sigmoid(val_logits).cpu().numpy()\n",
    "            val_true   = val_labels.cpu().numpy()\n",
    "\n",
    "            auc  = roc_auc_score(val_true, val_scores)\n",
    "            aupr = average_precision_score(val_true, val_scores)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Loss {loss.item():.4f} | \"\n",
    "            f\"Val AUC {auc:.4f} | \"\n",
    "            f\"Val AUPR {aupr:.4f}\"\n",
    "        )\n",
    "\n",
    "        if aupr > best_val_aupr:\n",
    "            best_val_aupr = aupr\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                OUT_DIR / f\"gat_best_model_seed{seed}.pth\"\n",
    "            )\n",
    "            print(\"   → Saved best model\")\n",
    "\n",
    "    # -------- Test --------\n",
    "    model.load_state_dict(\n",
    "        torch.load(OUT_DIR / f\"gat_best_model_seed{seed}.pth\", map_location=DEVICE)\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = model(data.x_dict, data.edge_index_dict)\n",
    "        circ_emb, dis_emb = emb[\"circRNA\"], emb[\"disease\"]\n",
    "\n",
    "        test_logits = (circ_emb[test_edges[0]] * dis_emb[test_edges[1]]).sum(dim=1)\n",
    "        test_scores = torch.sigmoid(test_logits).cpu().numpy()\n",
    "        test_true   = test_labels.cpu().numpy()\n",
    "\n",
    "        test_auc  = roc_auc_score(test_true, test_scores)\n",
    "        test_aupr = average_precision_score(test_true, test_scores)\n",
    "\n",
    "    print(f\"SEED {seed} | Test AUC {test_auc:.4f} | Test AUPR {test_aupr:.4f}\")\n",
    "    results.append((seed, test_auc, test_aupr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5533e338-78c3-4a6a-9263-0b49b85e232b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== FINAL SUMMARY ==========\n",
      "Seed 42: AUC=0.9243, AUPR=0.5617\n",
      "Seed 43: AUC=0.9403, AUPR=0.6851\n",
      "Seed 44: AUC=0.9227, AUPR=0.6061\n",
      "Seed 45: AUC=0.9349, AUPR=0.5920\n",
      "Seed 46: AUC=0.9223, AUPR=0.6335\n",
      "\n",
      "Mean ± Std over seeds\n",
      "AUC  : 0.9289 ± 0.0073\n",
      "AUPR : 0.6157 ± 0.0417\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n========== FINAL SUMMARY ==========\")\n",
    "\n",
    "for seed, auc, aupr in results:\n",
    "    print(f\"Seed {seed}: AUC={auc:.4f}, AUPR={aupr:.4f}\")\n",
    "\n",
    "aucs  = [r[1] for r in results]\n",
    "auprs = [r[2] for r in results]\n",
    "\n",
    "print(\"\\nMean ± Std over seeds\")\n",
    "print(f\"AUC  : {np.mean(aucs):.4f} ± {np.std(aucs):.4f}\")\n",
    "print(f\"AUPR : {np.mean(auprs):.4f} ± {np.std(auprs):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
